{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc8xUgZISmEN",
        "outputId": "e7583e45-c424-4277-cb3b-115b84451278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paUIolHiS6cA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6i86FM_Syb7",
        "outputId": "8266f08a-46ff-4482-b170-51bd4cca0c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "\n",
            " USING ONLY 20000 TRAINING EXAMPLES\n",
            "\n",
            "======================================================================\n",
            "APPROACH 1: Frozen Encoder + Logistic Regression\n",
            "======================================================================\n",
            "Loading FROZEN encoder: microsoft/unixcoder-base\n",
            "\n",
            "Extracting training embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|██████████| 625/625 [09:00<00:00,  1.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training classifier on 20000 examples...\n",
            "\n",
            "Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|██████████| 313/313 [04:30<00:00,  1.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Macro F1: 0.9307\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.93      0.93      0.93      4883\n",
            "          AI       0.93      0.93      0.93      5117\n",
            "\n",
            "    accuracy                           0.93     10000\n",
            "   macro avg       0.93      0.93      0.93     10000\n",
            "weighted avg       0.93      0.93      0.93     10000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|██████████| 32/32 [00:27<00:00,  1.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Macro F1: 0.4641\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.91      0.35      0.50       777\n",
            "          AI       0.28      0.88      0.43       223\n",
            "\n",
            "    accuracy                           0.47      1000\n",
            "   macro avg       0.60      0.62      0.46      1000\n",
            "weighted avg       0.77      0.47      0.49      1000\n",
            "\n",
            "\n",
            "======================================================================\n",
            "APPROACH 2: Feature-Based Classifier\n",
            "======================================================================\n",
            "Extracting training features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 100%|██████████| 20000/20000 [00:03<00:00, 6496.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training classifier...\n",
            "\n",
            "Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 100%|██████████| 10000/10000 [00:01<00:00, 5720.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Macro F1: 0.9405\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.95      0.93      0.94      4883\n",
            "          AI       0.93      0.95      0.94      5117\n",
            "\n",
            "    accuracy                           0.94     10000\n",
            "   macro avg       0.94      0.94      0.94     10000\n",
            "weighted avg       0.94      0.94      0.94     10000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 100%|██████████| 1000/1000 [00:00<00:00, 5836.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Macro F1: 0.3886\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.92      0.23      0.37       777\n",
            "          AI       0.26      0.93      0.40       223\n",
            "\n",
            "    accuracy                           0.39      1000\n",
            "   macro avg       0.59      0.58      0.39      1000\n",
            "weighted avg       0.77      0.39      0.38      1000\n",
            "\n",
            "\n",
            "======================================================================\n",
            "APPROACH 3: Ensemble (Feature-Based + Frozen Encoders)\n",
            "======================================================================\n",
            "\n",
            "Validating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|██████████| 313/313 [04:30<00:00,  1.16it/s]\n",
            "Extracting features: 100%|██████████| 10000/10000 [00:02<00:00, 4713.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Macro F1 (Ensemble): 0.9497\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.95      0.95      0.95      4883\n",
            "          AI       0.95      0.95      0.95      5117\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|██████████| 32/32 [00:27<00:00,  1.17it/s]\n",
            "Extracting features: 100%|██████████| 1000/1000 [00:00<00:00, 4468.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Macro F1 (Ensemble): 0.4523\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.94      0.32      0.47       777\n",
            "          AI       0.28      0.93      0.43       223\n",
            "\n",
            "    accuracy                           0.45      1000\n",
            "   macro avg       0.61      0.62      0.45      1000\n",
            "weighted avg       0.79      0.45      0.46      1000\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "Frozen Encoder:     0.4641\n",
            "Feature-Based:      0.3886\n",
            "Ensemble:           0.4523\n",
            "\n",
            "BEST MACRO F1: 0.4641\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# APPROACH 1: FROZEN ENCODER + LOGISTIC REGRESSION\n",
        "\n",
        "class FrozenEncoderClassifier:\n",
        "    def __init__(self, model_name=\"microsoft/unixcoder-base\", max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        print(f\"Loading FROZEN encoder: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.encoder = self.encoder.to(self.device)\n",
        "        self.encoder.eval()\n",
        "\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.classifier = None\n",
        "\n",
        "    def get_embeddings(self, texts, batch_size=32):\n",
        "        \"\"\"get embeddings without fine-tuning the encoder\"\"\"\n",
        "        all_embeddings = []\n",
        "\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                batch_texts,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                return_tensors='pt'\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.encoder(**inputs)\n",
        "                embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "            all_embeddings.append(embeddings)\n",
        "\n",
        "        return np.vstack(all_embeddings)\n",
        "\n",
        "    def train(self, train_texts, train_labels, val_texts=None, val_labels=None):\n",
        "        \"\"\"Train ONLY the classifier, NOT the encoder\"\"\"\n",
        "        print(\"\\nExtracting training embeddings...\")\n",
        "        train_embeddings = self.get_embeddings(train_texts)\n",
        "\n",
        "        print(f\"Training classifier on {len(train_embeddings)} examples...\")\n",
        "        self.classifier = LogisticRegression(\n",
        "            C=0.1,  \n",
        "            max_iter=1000,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        )\n",
        "        self.classifier.fit(train_embeddings, train_labels)\n",
        "\n",
        "        if val_texts is not None:\n",
        "            print(\"\\nValidating...\")\n",
        "            val_embeddings = self.get_embeddings(val_texts)\n",
        "            val_preds = self.classifier.predict(val_embeddings)\n",
        "            val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "            print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
        "            print(classification_report(val_labels, val_preds, target_names=['Human', 'AI']))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, texts):\n",
        "        \"\"\"Predict\"\"\"\n",
        "        embeddings = self.get_embeddings(texts)\n",
        "        return self.classifier.predict(embeddings)\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        \"\"\"Predict probabilities\"\"\"\n",
        "        embeddings = self.get_embeddings(texts)\n",
        "        return self.classifier.predict_proba(embeddings)\n",
        "\n",
        "# APPROACH 2: MANUAL FEATURE ENGINEERING\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "class CodeFeatureExtractor:\n",
        "    \"\"\"Extract universal features that distinguish AI from Human\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_features(code):\n",
        "        \"\"\"Features that do NOT depend on the specific language\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # 1. LENGTH AND STRUCTURE\n",
        "        features['length'] = len(code)\n",
        "        features['lines'] = code.count('\\n') + 1\n",
        "        features['avg_line_length'] = features['length'] / max(features['lines'], 1)\n",
        "\n",
        "        # 2. COMMENTS\n",
        "        features['comment_lines'] = len(re.findall(r'^\\s*[#//]', code, re.MULTILINE))\n",
        "        features['comment_ratio'] = features['comment_lines'] / max(features['lines'], 1)\n",
        "\n",
        "        # 3. WHITESPACE PATTERNS\n",
        "        lines = code.split('\\n')\n",
        "        indents = [len(line) - len(line.lstrip()) for line in lines if line.strip()]\n",
        "        features['avg_indent'] = np.mean(indents) if indents else 0\n",
        "        features['indent_std'] = np.std(indents) if indents else 0\n",
        "\n",
        "        # 4. NAMING PATTERNS\n",
        "        words = re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', code)\n",
        "        features['unique_words'] = len(set(words)) / max(len(words), 1)\n",
        "        features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0\n",
        "\n",
        "        # 5. SYNTACTIC COMPLEXITY\n",
        "        features['parentheses'] = code.count('(')\n",
        "        features['brackets'] = code.count('[')\n",
        "        features['braces'] = code.count('{')\n",
        "\n",
        "        # 6. SPECIFIC PATTERNS\n",
        "        features['has_docstring'] = int('\"\"\"' in code or \"'''\" in code)\n",
        "        features['has_type_hints'] = int(':' in code and '->' in code)\n",
        "\n",
        "        # 7. REPETITION\n",
        "        line_hashes = [hash(line.strip()) for line in lines if line.strip()]\n",
        "        features['repeated_lines'] = 1 - len(set(line_hashes)) / max(len(line_hashes), 1)\n",
        "\n",
        "        return features\n",
        "\n",
        "class FeatureBasedClassifier:\n",
        "    \"\"\"Classifier based on manual features\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_extractor = CodeFeatureExtractor()\n",
        "        self.classifier = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            min_samples_split=20,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    def extract_all_features(self, codes):\n",
        "        all_features = []\n",
        "        for code in tqdm(codes, desc=\"Extracting features\"):\n",
        "            features = self.feature_extractor.extract_features(code)\n",
        "            all_features.append(list(features.values()))\n",
        "        return np.array(all_features)\n",
        "\n",
        "    def train(self, train_codes, train_labels, val_codes=None, val_labels=None):\n",
        "        print(\"Extracting training features...\")\n",
        "        X_train = self.extract_all_features(train_codes)\n",
        "\n",
        "        print(\"Training classifier...\")\n",
        "        self.classifier.fit(X_train, train_labels)\n",
        "\n",
        "        if val_codes is not None:\n",
        "            print(\"\\nValidating...\")\n",
        "            X_val = self.extract_all_features(val_codes)\n",
        "            val_preds = self.classifier.predict(X_val)\n",
        "            val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "            print(f\"\\nValidation Macro F1: {val_f1:.4f}\")\n",
        "            print(classification_report(val_labels, val_preds, target_names=['Human', 'AI']))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, codes):\n",
        "        X = self.extract_all_features(codes)\n",
        "        return self.classifier.predict(X)\n",
        "\n",
        "    def predict_proba(self, codes):\n",
        "        X = self.extract_all_features(codes)\n",
        "        return self.classifier.predict_proba(X)\n",
        "\n",
        "# APPROACH 3: ENSEMBLE OF FROZEN MODELS\n",
        "\n",
        "class MultiModelFrozenEnsemble:\n",
        "    \"\"\"Ensemble of different FROZEN models\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = [\n",
        "            FrozenEncoderClassifier(\"microsoft/unixcoder-base\"),\n",
        "            FrozenEncoderClassifier(\"microsoft/graphcodebert-base\"),\n",
        "            FrozenEncoderClassifier(\"microsoft/codebert-base\"),\n",
        "        ]\n",
        "        self.weights = [0.4, 0.4, 0.2] \n",
        "\n",
        "    def train(self, train_texts, train_labels, train_size=20000):\n",
        "        \"\"\"Train all models on a small subset\"\"\"\n",
        "        indices = np.random.choice(len(train_texts), min(train_size, len(train_texts)), replace=False)\n",
        "        subset_texts = [train_texts[i] for i in indices]\n",
        "        subset_labels = [train_labels[i] for i in indices]\n",
        "\n",
        "        for i, model in enumerate(self.models):\n",
        "            print(f\"\\nTraining model {i+1}/{len(self.models)}\")\n",
        "            model.train(subset_texts, subset_labels)\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        \"\"\"Ensemble prediction\"\"\"\n",
        "        all_probs = []\n",
        "        for model, weight in zip(self.models, self.weights):\n",
        "            probs = model.predict_proba(texts)[:, 1]\n",
        "            all_probs.append(probs * weight)\n",
        "\n",
        "        return np.sum(all_probs, axis=0)\n",
        "\n",
        "    def predict(self, texts, threshold=0.5):\n",
        "        probs = self.predict_proba(texts)\n",
        "        return (probs >= threshold).astype(int)\n",
        "\n",
        "# MAIN: TRAINING AND EVALUATION\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = load_dataset(\"parquet\", data_files=TRAIN_PARQUET, split=\"train\")\n",
        "    val_dataset = load_dataset(\"parquet\", data_files=VAL_PARQUET, split=\"train\")\n",
        "    test_dataset = load_dataset(\"parquet\", data_files=TEST_PARQUET, split=\"train\")\n",
        "\n",
        "    TRAIN_SIZE = 20000\n",
        "\n",
        "    print(f\"\\n USING ONLY {TRAIN_SIZE} TRAINING EXAMPLES\")\n",
        "    train_subset = train_dataset.shuffle(seed=42).select(range(TRAIN_SIZE))\n",
        "\n",
        "    train_texts = [ex['code'] for ex in train_subset]\n",
        "    train_labels = [ex['label'] for ex in train_subset]\n",
        "\n",
        "    val_texts = [ex['code'] for ex in val_dataset.select(range(min(10000, len(val_dataset))))]\n",
        "    val_labels = [ex['label'] for ex in val_dataset.select(range(min(10000, len(val_dataset))))]\n",
        "\n",
        "    test_texts = [ex['code'] for ex in test_dataset]\n",
        "    test_labels = [ex['label'] for ex in test_dataset]\n",
        "\n",
        "    # FROZEN ENCODER + LOGISTIC REGRESSION\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"APPROACH 1: Frozen Encoder + Logistic Regression\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model1 = FrozenEncoderClassifier(\"microsoft/unixcoder-base\")\n",
        "    model1.train(train_texts, train_labels, val_texts, val_labels)\n",
        "\n",
        "    # Test\n",
        "    test_preds1 = model1.predict(test_texts)\n",
        "    test_f1_1 = f1_score(test_labels, test_preds1, average='macro')\n",
        "    print(f\"\\nTest Macro F1: {test_f1_1:.4f}\")\n",
        "    print(classification_report(test_labels, test_preds1, target_names=['Human', 'AI']))\n",
        "\n",
        "    # FEATURE-BASED\n",
        " \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"APPROACH 2: Feature-Based Classifier\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model2 = FeatureBasedClassifier()\n",
        "    model2.train(train_texts, train_labels, val_texts, val_labels)\n",
        "\n",
        "    # Test\n",
        "    test_preds2 = model2.predict(test_texts)\n",
        "    test_f1_2 = f1_score(test_labels, test_preds2, average='macro')\n",
        "    print(f\"\\nTest Macro F1: {test_f1_2:.4f}\")\n",
        "    print(classification_report(test_labels, test_preds2, target_names=['Human', 'AI']))\n",
        "\n",
        "    # ENSEMBLE\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"APPROACH 3: Ensemble (Feature-Based + Frozen Encoders)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Validation\n",
        "    print(\"\\nValidating...\")\n",
        "    val_probs1 = model1.predict_proba(val_texts)[:, 1]\n",
        "    val_probs2 = model2.predict_proba(val_texts)[:, 1]\n",
        "\n",
        "    val_ensemble_probs = 0.6 * val_probs1 + 0.4 * val_probs2\n",
        "    val_ensemble_preds = (val_ensemble_probs >= 0.5).astype(int)\n",
        "\n",
        "    val_f1_ensemble = f1_score(val_labels, val_ensemble_preds, average='macro')\n",
        "    print(f\"\\nValidation Macro F1 (Ensemble): {val_f1_ensemble:.4f}\")\n",
        "    print(classification_report(val_labels, val_ensemble_preds, target_names=['Human', 'AI']))\n",
        "\n",
        "    # Combine predictions\n",
        "    probs1 = model1.predict_proba(test_texts)[:, 1]\n",
        "    probs2 = model2.predict_proba(test_texts)[:, 1]\n",
        "\n",
        "    # Weighted average\n",
        "    ensemble_probs = 0.6 * probs1 + 0.4 * probs2\n",
        "    ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
        "\n",
        "    test_f1_ensemble = f1_score(test_labels, ensemble_preds, average='macro')\n",
        "    print(f\"\\nTest Macro F1 (Ensemble): {test_f1_ensemble:.4f}\")\n",
        "    print(classification_report(test_labels, ensemble_preds, target_names=['Human', 'AI']))\n",
        "\n",
        "    # BEST MODEL\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Frozen Encoder:     {test_f1_1:.4f}\")\n",
        "    print(f\"Feature-Based:      {test_f1_2:.4f}\")\n",
        "    print(f\"Ensemble:           {test_f1_ensemble:.4f}\")\n",
        "\n",
        "    best_f1 = max(test_f1_1, test_f1_2, test_f1_ensemble)\n",
        "    print(f\"\\nBEST MACRO F1: {best_f1:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    TRAIN_PARQUET = \"/content/drive/MyDrive/SemEval-2026-Task13/task_A/task_a_training_set_1.parquet\"\n",
        "    VAL_PARQUET = \"/content/drive/MyDrive/SemEval-2026-Task13/task_A/task_a_validation_set.parquet\"\n",
        "    TEST_PARQUET = \"/content/drive/MyDrive/SemEval-2026-Task13/task_A/task_a_test_set_sample.parquet\"\n",
        "\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
